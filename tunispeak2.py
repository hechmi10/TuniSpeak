# -*- coding: utf-8 -*-
"""TuniSpeak2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OxRMGCa9i2cilR1qc_CBp-qUIYTnSVAM
"""

!pip install xai-sdk --quiet
!pip install transformers torch sentencepiece langdetect PyPDF2 scikit-learn gradio fastapi uvicorn nest-asyncio arabic-reshaper python-bidi pyngrok arabert==1.0.1 nltk google-generativeai python-docx spacy textblob --quiet

import nest_asyncio
import os
import pandas as pd
import numpy as np
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
from transformers import pipeline
import langdetect
from arabert.preprocess import ArabertPreprocessor
import gradio as gr
from fastapi import FastAPI, Request, Form, File, UploadFile
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
import uvicorn
import threading
import asyncio
import requests
from pyngrok import ngrok
from google.colab import userdata
import nltk
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
from google.generativeai import types
import PyPDF2
from docx import Document
import tempfile
import json
from datetime import datetime
from collections import Counter, defaultdict

# Configuration initiale
print("ğŸ”§ Initialisation du systÃ¨me TuniSpeak avec NLP et ML...")

# Fetch GEMINI_API_KEY
GEMINI_API_KEY = None
try:
    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')
    if not GEMINI_API_KEY:
        print("âš ï¸ Warning: GEMINI_API_KEY secret not found. Generative AI fallback will not work.")
except Exception as e:
    print(f"âŒ Error fetching GEMINI_API_KEY secret: {e}")

# Download NLTK resources
try:
    nltk.download('punkt_tab')
    nltk.download('stopwords')
    nltk.download('averaged_perceptron_tagger')
    nltk.download('wordnet')
    print("âœ… NLTK resources downloaded")
except Exception as e:
    print(f"âš ï¸ Erreur lors du tÃ©lÃ©chargement des ressources NLTK : {e}")

# Patch pour exÃ©cuter uvicorn dans Colab
nest_asyncio.apply()

# =============================================================================
# MODULE D'APPRENTISSAGE - Collecte et analyse des interactions
# =============================================================================

class LearningModule:
    """Module d'apprentissage qui amÃ©liore le systÃ¨me au fil du temps"""

    def __init__(self):
        self.interactions_log = []
        self.feedback_log = []
        self.intent_classifier = None
        self.user_patterns = defaultdict(list)
        self.learning_data_file = "/content/learning_data.json"
        self.load_learning_data()

    def _get_language_name(self, code):
        """Convertit le code langue en nom complet"""
        language_map = {
            'fr': 'FranÃ§ais',
            'ar': 'Arabe',
            'dar': 'Darija Tunisienne'
        }
        return language_map.get(code, code)

    def load_learning_data(self):
        """Charge les donnÃ©es d'apprentissage prÃ©cÃ©dentes"""
        try:
            if os.path.exists(self.learning_data_file):
                with open(self.learning_data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.interactions_log = data.get('interactions', [])
                    self.feedback_log = data.get('feedback', [])
                print(f"âœ… DonnÃ©es d'apprentissage chargÃ©es: {len(self.interactions_log)} interactions")
        except Exception as e:
            print(f"âš ï¸ Impossible de charger les donnÃ©es d'apprentissage: {e}")

    def save_learning_data(self):
        """Sauvegarde les donnÃ©es d'apprentissage"""
        try:
            data = {
                'interactions': self.interactions_log[-1000:],  # Garder les 1000 derniÃ¨res
                'feedback': self.feedback_log[-500:]  # Garder les 500 derniers
            }
            with open(self.learning_data_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ Erreur sauvegarde apprentissage: {e}")

    def log_interaction(self, question, answer, source, language, similarity_score=None):
        """Enregistre une interaction pour l'apprentissage"""
        interaction = {
            'timestamp': datetime.now().isoformat(),
            'question': question,
            'answer': answer[:200],  # Tronquer pour Ã©conomiser l'espace
            'source': source,
            'language': language,
            'similarity_score': similarity_score
        }
        self.interactions_log.append(interaction)

        # Sauvegarder tous les 10 interactions
        if len(self.interactions_log) % 10 == 0:
            self.save_learning_data()

    def log_feedback(self, question, was_helpful, comment=None):
        """Enregistre le feedback utilisateur"""
        feedback = {
            'timestamp': datetime.now().isoformat(),
            'question': question,
            'was_helpful': was_helpful,
            'comment': comment
        }
        self.feedback_log.append(feedback)
        self.save_learning_data()

    def analyze_patterns(self):
        """Analyse les patterns d'utilisation"""
        if len(self.interactions_log) == 0:
            return "ğŸ“Š **Rapport d'Apprentissage**\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nAucune donnÃ©e disponible. Posez quelques questions pour gÃ©nÃ©rer un rapport !"

        # Analyse toujours, mÃªme avec peu de donnÃ©es
        languages = [i['language'] for i in self.interactions_log]
        lang_counts = Counter(languages)

        sources = [i['source'] for i in self.interactions_log]
        source_counts = Counter(sources)

        # Calculer les scores de similaritÃ© (uniquement pour FAQ)
        faq_scores = [i['similarity_score'] for i in self.interactions_log
                      if i['source'] == 'FAQ' and i['similarity_score'] is not None]
        avg_faq_score = np.mean(faq_scores) if faq_scores else 0

        # Calculer les statistiques de feedback
        helpful_feedback = [f for f in self.feedback_log if f['was_helpful']]
        helpful_rate = len(helpful_feedback) / len(self.feedback_log) * 100 if self.feedback_log else 0

        # CrÃ©er le rapport
        report_lines = [
            "ğŸ“Š **Rapport d'Apprentissage**",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
            f"ğŸ“ˆ Total interactions: {len(self.interactions_log)}",
            f"ğŸ’¬ Feedbacks reÃ§us: {len(self.feedback_log)}",
            f"ğŸ‘ Taux de satisfaction: {helpful_rate:.1f}%",
            "",
            "ğŸŒ **Distribution des langues:**"
        ]

        # Ajouter les langues
        for lang, count in lang_counts.most_common():
            percentage = count / len(languages) * 100
            report_lines.append(f"  â€¢ {self._get_language_name(lang)}: {count} ({percentage:.1f}%)")

        report_lines.append("")
        report_lines.append("ğŸ“š **Sources utilisÃ©es:**")

       # Ajouter les sources
        for source, count in source_counts.most_common():
            percentage = count / len(sources) * 100
            report_lines.append(f"  â€¢ {source}: {count} ({percentage:.1f}%)")

       # Ajouter les performances si pertinent
        if faq_scores:
           report_lines.append("")
           report_lines.append("ğŸ¯ **Performance FAQ:**")
           report_lines.append(f"  â€¢ Score de similaritÃ© moyen: {avg_faq_score:.3f}")
           report_lines.append(f"  â€¢ Nombre de rÃ©ponses FAQ: {len(faq_scores)}")

    # Ajouter des conseils si peu de donnÃ©es
        if len(self.interactions_log) < 10:
          report_lines.append("")
          report_lines.append("ğŸ’¡ **Conseil:** Continuez Ã  poser des questions pour obtenir une analyse plus prÃ©cise!")

        return "\n".join(report_lines)

    def train_intent_classifier(self, questions, intents):
        """EntraÃ®ne un classificateur d'intentions"""
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.naive_bayes import MultinomialNB
            from sklearn.pipeline import Pipeline

            # CrÃ©er un pipeline
            self.intent_classifier = Pipeline([
                ('tfidf', TfidfVectorizer(max_features=1000)),
                ('clf', MultinomialNB())
            ])

            # EntraÃ®ner
            X_train, X_test, y_train, y_test = train_test_split(
                questions, intents, test_size=0.2, random_state=42
            )

            self.intent_classifier.fit(X_train, y_train)

            # Ã‰valuer
            y_pred = self.intent_classifier.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)

            print(f"âœ… Classificateur d'intentions entraÃ®nÃ© (Accuracy: {accuracy:.3f})")
            return accuracy

        except Exception as e:
            print(f"âŒ Erreur entraÃ®nement classificateur: {e}")
            return 0

    def predict_intent(self, question):
        """PrÃ©dit l'intention d'une question"""
        if self.intent_classifier:
            try:
                return self.intent_classifier.predict([question])[0]
            except:
                return "unknown"
        return "unknown"

# Initialiser le module d'apprentissage
learning_module = LearningModule()

# =============================================================================
# MODULE NLP AVANCÃ‰ - Analyse linguistique approfondie
# =============================================================================

class NLPModule:
    """Module NLP pour analyse linguistique avancÃ©e"""

    def __init__(self):
        self.sentiment_analyzer = None
        self.ner_model = None
        self.init_models()

    def init_models(self):
        """Initialise les modÃ¨les NLP"""
        try:
            # Analyseur de sentiment multilingue
            self.sentiment_analyzer = pipeline(
                "sentiment-analysis",
                model="nlptown/bert-base-multilingual-uncased-sentiment"
            )
            print("âœ… Analyseur de sentiment chargÃ©")
        except Exception as e:
            print(f"âš ï¸ Sentiment analyzer non disponible: {e}")

        try:
            # NER (Named Entity Recognition)
            self.ner_model = pipeline(
                "ner",
                model="Davlan/distilbert-base-multilingual-cased-ner-hrl"
            )
            print("âœ… ModÃ¨le NER chargÃ©")
        except Exception as e:
            print(f"âš ï¸ NER model non disponible: {e}")

    def analyze_sentiment(self, text):
        """Analyse le sentiment du texte"""
        if not self.sentiment_analyzer:
            return {"label": "NEUTRAL", "score": 0.5}

        try:
            result = self.sentiment_analyzer(text[:512])[0]
            return result
        except Exception as e:
            print(f"âš ï¸ Erreur analyse sentiment: {e}")
            return {"label": "NEUTRAL", "score": 0.5}

    def extract_entities(self, text):
        """Extrait les entitÃ©s nommÃ©es"""
        if not self.ner_model:
            return []

        try:
            entities = self.ner_model(text[:512])
            # Grouper les entitÃ©s
            grouped = []
            current_entity = None

            for ent in entities:
                if ent['entity'].startswith('B-'):
                    if current_entity:
                        grouped.append(current_entity)
                    current_entity = {
                        'text': ent['word'],
                        'type': ent['entity'][2:],
                        'score': ent['score']
                    }
                elif ent['entity'].startswith('I-') and current_entity:
                    current_entity['text'] += ' ' + ent['word']
                    current_entity['score'] = (current_entity['score'] + ent['score']) / 2

            if current_entity:
                grouped.append(current_entity)

            return grouped
        except Exception as e:
            print(f"âš ï¸ Erreur extraction entitÃ©s: {e}")
            return []

    def extract_keywords(self, text, top_n=5):
        """Extrait les mots-clÃ©s importants"""
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer

            # Tokeniser les phrases
            sentences = nltk.sent_tokenize(text)
            if len(sentences) < 2:
                return []

            # TF-IDF
            vectorizer = TfidfVectorizer(max_features=top_n, stop_words='english')
            tfidf_matrix = vectorizer.fit_transform(sentences)

            # Obtenir les mots-clÃ©s
            feature_names = vectorizer.get_feature_names_out()
            scores = tfidf_matrix.sum(axis=0).A1

            keywords = [(feature_names[i], scores[i]) for i in range(len(feature_names))]
            keywords.sort(key=lambda x: x[1], reverse=True)

            return [k[0] for k in keywords[:top_n]]
        except Exception as e:
            print(f"âš ï¸ Erreur extraction keywords: {e}")
            return []

    def analyze_text_complexity(self, text):
        """Analyse la complexitÃ© du texte"""
        try:
            words = text.split()
            sentences = nltk.sent_tokenize(text)

            avg_word_length = np.mean([len(word) for word in words])
            avg_sentence_length = len(words) / len(sentences) if sentences else 0

            # Score de complexitÃ© simple
            complexity_score = (avg_word_length * 0.5 + avg_sentence_length * 0.1) / 2

            if complexity_score < 3:
                level = "Simple"
            elif complexity_score < 5:
                level = "Moyen"
            else:
                level = "Complexe"

            return {
                'level': level,
                'avg_word_length': round(avg_word_length, 2),
                'avg_sentence_length': round(avg_sentence_length, 2),
                'complexity_score': round(complexity_score, 2)
            }
        except Exception as e:
            print(f"âš ï¸ Erreur analyse complexitÃ©: {e}")
            return {'level': 'Unknown', 'complexity_score': 0}

    def full_analysis(self, text):
        """Analyse NLP complÃ¨te"""
        analysis = {
            'sentiment': self.analyze_sentiment(text),
            'entities': self.extract_entities(text),
            'keywords': self.extract_keywords(text),
            'complexity': self.analyze_text_complexity(text)
        }
        return analysis

# Initialiser le module NLP
nlp_module = NLPModule()

# =============================================================================
# CHARGEMENT DES DONNÃ‰ES FAQ
# =============================================================================

def load_faq_data():
    """Charge les donnÃ©es FAQ depuis le fichier CSV fourni"""
    try:
        faq_df = pd.read_csv('tuni_speak_faq_sample.csv')
        faq_df.fillna("", inplace=True)
        faq_df['text'] = faq_df.apply(
            lambda row: f"Question: {row['question']} RÃ©ponse: {row['answer']}",
            axis=1
        )
        print(f"âœ… DonnÃ©es FAQ chargÃ©es: {len(faq_df)} entrÃ©es")
        print(f"ğŸŒ Langues disponibles: {faq_df['language'].unique()}")
        return faq_df
    except Exception as e:
        print(f"âŒ Erreur lors du chargement des donnÃ©es FAQ: {e}")
        return pd.DataFrame(columns=['id', 'language', 'question', 'answer', 'text'])

faq_df = load_faq_data()

# =============================================================================
# INITIALISATION DES MODÃˆLES
# =============================================================================

model_name = "deepset/xlm-roberta-large-squad2"
try:
    qa_pipeline = pipeline("question-answering", model=model_name, tokenizer=model_name)
    print("âœ… ModÃ¨le QA chargÃ© avec succÃ¨s")
except Exception as e:
    print(f"âŒ Erreur lors du chargement du modÃ¨le QA : {e}")
    qa_pipeline = None

try:
    arabert_prep = ArabertPreprocessor(model_name="aubmindlab/bert-base-arabertv02")
    print("âœ… PrÃ©processeur AraBERT chargÃ©")
except Exception as e:
    print(f"âš ï¸ Arabert non disponible, utilisant normalisation basique. Erreur : {e}")
    def arabert_prep_fallback(text):
        return text.lower().strip()
    arabert_prep = arabert_prep_fallback

try:
    embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

    if not faq_df.empty:
        faq_df['normalized_question'] = faq_df.apply(
            lambda row: normalize_text(row['question'], row['language']),
            axis=1
        )
        faq_df['question_embeddings'] = faq_df['normalized_question'].apply(
            lambda x: embedding_model.encode(x) if x and x != "vide" else embedding_model.encode("")
        )
        print("âœ… Embeddings crÃ©Ã©s pour la recherche sÃ©mantique")
    else:
        print("âš ï¸ Aucune donnÃ©e FAQ pour crÃ©er les embeddings")
except Exception as e:
    print(f"âŒ Erreur lors de la crÃ©ation des embeddings: {e}")
    embedding_model = None

# =============================================================================
# FONCTIONS DE BASE
# =============================================================================

def detect_language(text):
    """DÃ©tecte le langage avec meilleure dÃ©tection du darija"""
    try:
        if not text or not isinstance(text, str):
            return "fr"

        arabic_chars = re.compile(r'[\u0600-\u06FF]')
        darija_patterns = [
            r'\b(ch|sh|9|3|7|khouya|lazma|chnouwa|kifech|n3mel|bach|ntsajel|m3ak|fama|wallah|yasser)\b',
            r'\b(ya3tik|saha|yais|mouch|mch|ken|walah|hata|waktah)\b',
            r'\b(bara|tawa|famma|kima|qadda|mta3|bech|t7eb)\b'
        ]

        if arabic_chars.search(text):
            text_lower = text.lower()
            for pattern in darija_patterns:
                if re.search(pattern, text_lower):
                    return "dar"
            return "ar"

        return langdetect.detect(text) if text and text.strip() else "fr"
    except Exception:
        return "fr"

def normalize_text(text, lang):
    """Nettoie et normalise le texte selon la langue"""
    if not text or not isinstance(text, str):
        return "vide"
    text = text.replace("\n", " ").strip()
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r'[^\w\s]', '', text)
    if lang in ["fr", "dar"]:
        text = text.lower()
    if lang in ["ar", "dar"] and callable(getattr(arabert_prep, 'preprocess', None)):
        text = arabert_prep.preprocess(text)
    return text if text else "vide"

# =============================================================================
# FONCTIONS DE RECHERCHE ET RÃ‰PONSE AMÃ‰LIORÃ‰ES
# =============================================================================

def find_similar_faq(query, threshold=0.6):
    """Recherche les FAQ similaires"""
    if not query or not isinstance(query, str) or faq_df.empty or embedding_model is None:
        return None, None, None, 0.0

    lang = detect_language(query)
    normalized_query = normalize_text(query, lang)

    try:
        query_embedding = embedding_model.encode(normalized_query)
        query_embedding = query_embedding.reshape(1, -1)

        faq_embeddings_list = [np.array(emb) for emb in faq_df['question_embeddings'].tolist()]
        if not faq_embeddings_list:
            return None, None, None, 0.0

        faq_embeddings_matrix = np.vstack(faq_embeddings_list)
        similarities = cosine_similarity(query_embedding, faq_embeddings_matrix).flatten()
        most_similar_index = np.argmax(similarities)
        similarity_score = similarities[most_similar_index]

        if similarity_score >= threshold:
            most_similar_faq = faq_df.iloc[most_similar_index]
            return (most_similar_faq['question'],
                    most_similar_faq['answer'],
                    most_similar_faq['language'],
                    float(similarity_score))
        else:
            return None, None, None, similarity_score

    except Exception as e:
        print(f"âŒ Error in find_similar_faq: {e}")
        return None, None, None, 0.0

def configure_gemini_api(api_key):
    """Configure la clÃ© API Gemini"""
    global GEMINI_API_KEY
    GEMINI_API_KEY = api_key.strip()
    if GEMINI_API_KEY:
        try:
            genai.configure(api_key=GEMINI_API_KEY)
            # Tester la connexion
            test_model = genai.GenerativeModel('gemini-flash-latest')
            test_response = test_model.generate_content("Test")
            return f"âœ… ClÃ© API configurÃ©e avec succÃ¨s ! ModÃ¨le testÃ©: {test_model.model_name}"
        except Exception as e:
            return f"âŒ Erreur de configuration: {str(e)}"
    else:
        return "âš ï¸ Veuillez entrer une clÃ© API valide"

def call_generative_ai(query):
    """IA gÃ©nÃ©rative qui rÃ©pond systÃ©matiquement en 3 langues"""
    if not GEMINI_API_KEY:
        return "ğŸ‡¹ğŸ‡³ Ø§Ù„Ø¯Ø§Ø±Ø¬Ø© Ø§Ù„ØªÙˆÙ†Ø³ÙŠØ©: Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù…ÙÙŠØ´ Ø¥Ø¬Ø§Ø¨Ø© Ø­Ø§Ø¶Ø±Ø©.\nğŸ‡¸ğŸ‡¦ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰: Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠÙ‹Ø§.\nğŸ‡«ğŸ‡· FranÃ§ais: DÃ©solÃ©, aucune rÃ©ponse disponible pour le moment. (ClÃ© API non configurÃ©e)", "IA GÃ©nÃ©rative (ClÃ© API manquante)"

    try:
        genai.configure(api_key=GEMINI_API_KEY)

        system_prompt = """Tu es un assistant trilingue spÃ©cialisÃ©.
        Pour CHAQUE question, tu DOIS rÃ©pondre en 3 langues dans cet ordre :
        1. Darija tunisienne (Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„ØªÙˆÙ†Ø³ÙŠØ©)
        2. Arabe standard (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰)
        3. FranÃ§ais

        Format de rÃ©ponse OBLIGATOIRE :
        ğŸ‡¹ğŸ‡³ Ø§Ù„Ø¯Ø§Ø±Ø¬Ø© Ø§Ù„ØªÙˆÙ†Ø³ÙŠØ©: [rÃ©ponse en darija]
        ğŸ‡¸ğŸ‡¦ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰: [rÃ©ponse en arabre standard]
        ğŸ‡«ğŸ‡· FranÃ§ais: [rÃ©ponse en franÃ§ais]

        Sois concis et prÃ©cis dans tes rÃ©ponses. Si tu ne connais pas la rÃ©ponse, dis-le clairement."""

        # Utiliser un modÃ¨le plus stable
        try:
            gemini_model = genai.GenerativeModel(
                model_name='gemini-flash-latest',
                system_instruction=system_prompt
            )
        except:
            # Fallback Ã  un autre modÃ¨le
            gemini_model = genai.GenerativeModel(
                model_name='gemini-pro',
                system_instruction=system_prompt
            )

        # Configuration de sÃ©curitÃ©
        generation_config = {
            "temperature": 0.7,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 1024,
        }

        response = gemini_model.generate_content(
            query,
            generation_config=generation_config,
            safety_settings={
                types.HarmCategory.HARM_CATEGORY_HARASSMENT: types.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                types.HarmCategory.HARM_CATEGORY_HATE_SPEECH: types.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            }
        )

        if response.text:
            return response.text.strip(), "IA GÃ©nÃ©rative (Gemini)"
        else:
            return "ğŸ‡¹ğŸ‡³ Ø§Ù„Ø¯Ø§Ø±Ø¬Ø© Ø§Ù„ØªÙˆÙ†Ø³ÙŠØ©: Ù…ÙÙŠØ´ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„Ø³ÙŠØ±Ú¤Ø±.\nğŸ‡¸ğŸ‡¦ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰: Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„Ø®Ø§Ø¯Ù….\nğŸ‡«ğŸ‡· FranÃ§ais: Pas de rÃ©ponse du serveur.", "IA GÃ©nÃ©rative (Erreur)"

    except Exception as e:
        print(f"âŒ Erreur dÃ©taillÃ©e lors de l'appel Ã  l'IA gÃ©nÃ©rative: {e}")
        return f"""ğŸ‡¹ğŸ‡³ Ø§Ù„Ø¯Ø§Ø±Ø¬Ø© Ø§Ù„ØªÙˆÙ†Ø³ÙŠØ©: Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØµÙŠØ§Ù†Ø© Ø§Ù„ØªÙ‚Ù†ÙŠØ©.
ğŸ‡¸ğŸ‡¦ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰: Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø§Ù„Ø®Ø¯Ù…Ø© Ù‚ÙŠØ¯ Ø§Ù„ØµÙŠØ§Ù†Ø© Ø§Ù„ØªÙ‚Ù†ÙŠØ©.
ğŸ‡«ğŸ‡· FranÃ§ais: DÃ©solÃ©, le service est en maintenance technique.

Erreur: {str(e)[:100]}""", "IA GÃ©nÃ©rative (Erreur)"

def answer_question(query, include_nlp_analysis=False):
    """Fonction principale pour rÃ©pondre aux questions avec analyse NLP optionnelle"""
    if not query or not isinstance(query, str):
        return "Veuillez entrer une question valide.", "N/A", None

    print(f"\nğŸ” Traitement de la question: {query}")
    lang = detect_language(query)
    print(f"ğŸŒ Langue dÃ©tectÃ©e: {lang}")

    # Analyse NLP si demandÃ©e
    nlp_analysis = None
    if include_nlp_analysis:
        nlp_analysis = nlp_module.full_analysis(query)
        print(f"ğŸ§  Analyse NLP effectuÃ©e")

    # VÃ©rifier dans les FAQ
    if not faq_df.empty and embedding_model is not None:
        faq_question, faq_answer, faq_lang, faq_similarity_score = find_similar_faq(query, threshold=0.6)

        if faq_question and faq_similarity_score > 0.6:
            print(f"âœ… FAQ similaire trouvÃ©e (Score: {faq_similarity_score:.3f})")

            # Logger l'interaction
            learning_module.log_interaction(query, faq_answer, "FAQ", lang, faq_similarity_score)

            return faq_answer, f"FAQ (Score: {faq_similarity_score:.3f})", nlp_analysis

        print(f"âš ï¸ SimilaritÃ© FAQ insuffisante ({faq_similarity_score:.3f})")

    # Utiliser l'IA gÃ©nÃ©rative
    print("ğŸ¤– Utilisation de l'IA gÃ©nÃ©rative...")
    answer = call_generative_ai(query)

    # Logger l'interaction
    learning_module.log_interaction(query, answer, "IA GÃ©nÃ©rative", lang)

    return answer, "IA GÃ©nÃ©rative", nlp_analysis

# =============================================================================
# FONCTIONS POUR LES DOCUMENTS
# =============================================================================

def extract_text_from_pdf(file_path):
    """Extraction des PDF"""
    text = ""
    try:
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            for page_num in range(len(reader.pages)):
                page_text = reader.pages[page_num].extract_text() or ""
                page_text = re.sub(r'\s+', ' ', page_text)
                page_text = re.sub(r'\\n', ' ', page_text)
                text += page_text + " "
    except Exception as e:
        print(f"âŒ Erreur extraction PDF {file_path}: {e}")
        return ""
    return text.strip()

def extract_text_from_docx(file_path):
    """Extraction des DOCX"""
    text = []
    try:
        document = Document(file_path)
        for paragraph in document.paragraphs:
            if paragraph.text.strip():
                text.append(paragraph.text)
    except Exception as e:
        print(f"âŒ Erreur extraction DOCX {file_path}: {e}")
        return ""
    return '\n'.join(text)

def extract_text(file_path):
    """Dispatch l'extraction selon le type de fichier"""
    file_extension = os.path.splitext(file_path)[1].lower()
    if file_extension == '.pdf':
        return extract_text_from_pdf(file_path)
    elif file_extension == '.docx':
        return extract_text_from_docx(file_path)
    else:
        print(f"âš ï¸ Type de fichier non supportÃ©: {file_extension}")
        return ""

# =============================================================================
# INTERFACE GRADIO AMÃ‰LIORÃ‰E
# =============================================================================

def gradio_interface(question, uploaded_file, enable_nlp_analysis, show_learning_report):
    """Interface Gradio avec NLP et apprentissage"""
    response_parts = []

    # Afficher le rapport d'apprentissage si demandÃ©
    if show_learning_report:
        report = learning_module.analyze_patterns()
        response_parts.append(report)
        return "\n\n".join(response_parts)

    # Traitement du document
    if uploaded_file is not None:
        print(f"ğŸ“„ Traitement du document: {uploaded_file.name}")
        try:
            if isinstance(uploaded_file, str):
                file_path = uploaded_file
                file_name = os.path.basename(file_path)
            else:
                file_path = uploaded_file.name if hasattr(uploaded_file, 'name') else str(uploaded_file)
                file_name = os.path.basename(file_path)

            new_text_content = extract_text(file_path)

            if new_text_content and len(new_text_content.strip()) > 50:
                response_parts.append(f"**âœ… Document '{file_name}' traitÃ© avec succÃ¨s.**")
                response_parts.append(f"**ğŸ“Š Contenu extrait :** {len(new_text_content)} caractÃ¨res")

                # Analyse NLP du document si activÃ©e
                if enable_nlp_analysis:
                    doc_analysis = nlp_module.full_analysis(new_text_content[:1000])
                    response_parts.append("\n**ğŸ§  Analyse NLP du document :**")
                    response_parts.append(f"â€¢ Sentiment: {doc_analysis['sentiment']['label']}")
                    response_parts.append(f"â€¢ ComplexitÃ©: {doc_analysis['complexity']['level']}")
                    if doc_analysis['keywords']:
                        response_parts.append(f"â€¢ Mots-clÃ©s: {', '.join(doc_analysis['keywords'])}")

                print(f"âœ… Document intÃ©grÃ©: {len(new_text_content)} caractÃ¨res")
            else:
                response_parts.append(f"**âŒ Erreur :** Document vide ou impossible Ã  extraire.")

            os.unlink(file_path)

        except Exception as e:
            response_parts.append(f"**âŒ Erreur traitement fichier :** {str(e)}")

    # Traitement de la question
    if question and question.strip():
        print(f"â“ Question reÃ§ue: {question}")
        answer, source, nlp_analysis = answer_question(question, enable_nlp_analysis)

        response_parts.append(f"**â“ Votre question :** {question}")
        response_parts.append(f"**ğŸ’¡ RÃ©ponse :** {answer}")
        response_parts.append(f"**ğŸ“š Source :** {source}")

        # Afficher l'analyse NLP si disponible
        if nlp_analysis and enable_nlp_analysis:
            response_parts.append("\n**ğŸ§  Analyse NLP de votre question :**")
            response_parts.append(f"â€¢ Sentiment: {nlp_analysis['sentiment']['label']} (Score: {nlp_analysis['sentiment']['score']:.3f})")
            response_parts.append(f"â€¢ ComplexitÃ©: {nlp_analysis['complexity']['level']}")

            if nlp_analysis['entities']:
                entities_str = ', '.join([f"{e['text']} ({e['type']})" for e in nlp_analysis['entities']])
                response_parts.append(f"â€¢ EntitÃ©s dÃ©tectÃ©es: {entities_str}")

            if nlp_analysis['keywords']:
                response_parts.append(f"â€¢ Mots-clÃ©s: {', '.join(nlp_analysis['keywords'])}")

        print(f"âœ… RÃ©ponse gÃ©nÃ©rÃ©e - Source: {source}")

    if not response_parts:
        response_parts.append("**â„¹ï¸ Veuillez :**\nâ€¢ Poser une question\nâ€¢ OU uploader un document\nâ€¢ OU voir le rapport d'apprentissage !")

    return "\n\n".join(response_parts)

# CrÃ©er l'interface Gradio
print("ğŸš€ CrÃ©ation de l'interface Gradio amÃ©liorÃ©e...")
interface = gr.Interface(
    fn=gradio_interface,
    inputs=[
        gr.Textbox(
            label="Posez votre question",
            placeholder="Ex: Comment m'inscrire Ã  l'universitÃ© ?",
            lines=2
        ),
        gr.File(
            label="ğŸ“¤ Uploader un document (PDF/DOCX)",
            file_types=[".pdf", ".docx"]
        ),
        gr.Checkbox(
            label="ğŸ§  Activer l'analyse NLP avancÃ©e",
            value=False
        ),
        gr.Checkbox(
            label="ğŸ“Š Afficher le rapport d'apprentissage",
            value=False
        )
    ],
    outputs=gr.Textbox(
        label="RÃ©ponse du systÃ¨me",
        lines=15,
        show_copy_button=True
    ),
    title="ğŸ‡¹ğŸ‡³ TuniSpeak - Assistant Trilingue avec IA & Apprentissage",
    description=f"""
    **âœ¨ VERSION COMPLÃˆTE avec NLP et Machine Learning âœ¨**
    âœ“ {len(faq_df)} questions FAQ chargÃ©es
    âœ“ Analyse NLP avancÃ©e (sentiment, entitÃ©s, mots-clÃ©s)
    âœ“ Module d'apprentissage continu
    âœ“ {len(learning_module.interactions_log)} interactions enregistrÃ©es
    âœ“ Support trilingue (franÃ§ais, arabe, darija)
    """,
    examples=[
        ["Comment m'inscrire Ã  l'universitÃ© ?", None, True, False],
        ["ÙƒÙŠÙ Ø£Ø³Ø¬Ù„ ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©ØŸ", None, False, False],
        ["Quels sont les frais de scolaritÃ© ?", None, True, False],
        ["", None, False, True]  # Voir le rapport
    ]
)

# =============================================================================
# APPLICATION FASTAPI AMÃ‰LIORÃ‰E
# =============================================================================

app = FastAPI(title="TuniSpeak API", version="3.0")

os.makedirs("/content/templates", exist_ok=True)

html_content = '''
<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ‡¹ğŸ‡³ TuniSpeak - Assistant Trilingue IA</title>
    <style>
        * { box-sizing: border-box; margin: 0; padding: 0; }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }
        .container {
            max-width: 1000px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
        }
        h1 {
            color: #2c3e50;
            text-align: center;
            margin-bottom: 10px;
        }
        .subtitle {
            text-align: center;
            color: #7f8c8d;
            margin-bottom: 20px;
        }
        .badge {
            display: inline-block;
            background: #3498db;
            color: white;
            padding: 5px 12px;
            border-radius: 15px;
            font-size: 12px;
            margin: 0 5px;
        }
        .stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }
        .stat-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
        }
        .stat-number {
            font-size: 32px;
            font-weight: bold;
            margin: 10px 0;
        }
        .stat-label {
            font-size: 14px;
            opacity: 0.9;
        }
        .input-group {
            margin-bottom: 20px;
        }
        label {
            display: block;
            margin-bottom: 8px;
            font-weight: 600;
            color: #2c3e50;
        }
        textarea, input[type="file"] {
            width: 100%;
            padding: 12px;
            border: 2px solid #ddd;
            border-radius: 8px;
            font-size: 16px;
            transition: border-color 0.3s;
        }
        textarea:focus, input[type="file"]:focus {
            border-color: #3498db;
            outline: none;
        }
        textarea {
            height: 120px;
            resize: vertical;
        }
        .checkbox-group {
            margin: 15px 0;
        }
        .checkbox-label {
            display: flex;
            align-items: center;
            margin: 10px 0;
            cursor: pointer;
        }
        .checkbox-label input {
            margin-right: 10px;
            width: 20px;
            height: 20px;
            cursor: pointer;
        }
        button {
            background: #3498db;
            color: white;
            border: none;
            padding: 15px 30px;
            font-size: 18px;
            border-radius: 8px;
            cursor: pointer;
            width: 100%;
            transition: background 0.3s;
            margin-top: 10px;
        }
        button:hover {
            background: #2980b9;
        }
        button.secondary {
            background: #95a5a6;
        }
        button.secondary:hover {
            background: #7f8c8d;
        }
        #response {
            margin-top: 30px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 8px;
            border-left: 4px solid #3498db;
            white-space: pre-wrap;
            max-height: 600px;
            overflow-y: auto;
        }
        #loading {
            display: none;
            text-align: center;
            padding: 20px;
            color: #3498db;
        }
        .features {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }
        .feature {
            text-align: center;
            padding: 15px;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid #e9ecef;
        }
        .feature-icon {
            font-size: 24px;
            margin-bottom: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ‡¹ğŸ‡³ TuniSpeak Assistant IA</h1>
        <div class="subtitle">
            <span class="badge">ğŸ§  NLP</span>
            <span class="badge">ğŸ“Š ML</span>
            <span class="badge">ğŸŒ Trilingue</span>
        </div>

        <div class="stats">
            <div class="stat-card">
                <div class="stat-label">Questions FAQ</div>
                <div class="stat-number">{{ faq_count }}</div>
            </div>
            <div class="stat-card">
                <div class="stat-label">Interactions</div>
                <div class="stat-number">{{ interactions_count }}</div>
            </div>
            <div class="stat-card">
                <div class="stat-label">Feedbacks</div>
                <div class="stat-number">{{ feedback_count }}</div>
            </div>
        </div>

        <div class="features">
            <div class="feature">
                <div class="feature-icon">ğŸ§ </div>
                <div>Analyse NLP</div>
            </div>
            <div class="feature">
                <div class="feature-icon">ğŸ“š</div>
                <div>Base FAQ</div>
            </div>
            <div class="feature">
                <div class="feature-icon">ğŸ“„</div>
                <div>Documents</div>
            </div>
            <div class="feature">
                <div class="feature-icon">ğŸ“Š</div>
                <div>Apprentissage</div>
            </div>
        </div>

        <div class="input-group">
            <label for="question">ğŸ’¬ Votre question :</label>
            <textarea id="question" placeholder="Posez votre question en franÃ§ais, arabe ou darija..."></textarea>
        </div>

        <div class="input-group">
            <label for="fileUpload">ğŸ“¤ Document (PDF/DOCX) :</label>
            <input type="file" id="fileUpload" accept=".pdf,.docx">
        </div>

        <div class="checkbox-group">
            <label class="checkbox-label">
                <input type="checkbox" id="enableNLP">
                ğŸ§  Activer l'analyse NLP avancÃ©e
            </label>
        </div>

        <button onclick="submitQuestion()">ğŸš€ Envoyer</button>
        <button class="secondary" onclick="showLearningReport()">ğŸ“Š Rapport d'Apprentissage</button>

        <div id="loading">
            <div>â³ Traitement en cours...</div>
        </div>

        <div id="response"></div>
    </div>

    <script>
        async function submitQuestion() {
            const question = document.getElementById("question").value;
            const fileInput = document.getElementById("fileUpload");
            const file = fileInput.files[0];
            const enableNLP = document.getElementById("enableNLP").checked;
            const responseDiv = document.getElementById("response");
            const loadingDiv = document.getElementById("loading");

            if (!question.trim() && !file) {
                responseDiv.innerHTML = "<strong>âŒ Attention :</strong> Veuillez poser une question ou uploader un document.";
                return;
            }

            loadingDiv.style.display = "block";
            responseDiv.innerHTML = "";

            const formData = new FormData();
            if (question.trim()) formData.append("question", question);
            if (file) formData.append("uploaded_file", file);
            formData.append("enable_nlp", enableNLP);

            try {
                const response = await fetch("/qa", {
                    method: "POST",
                    body: formData
                });

                if (!response.ok) throw new Error(`HTTP ${response.status}`);

                const data = await response.json();
                let htmlResponse = data.answer.replace(/\\n/g, '<br>');
                responseDiv.innerHTML = htmlResponse;

            } catch (error) {
                console.error("Error:", error);
                responseDiv.innerHTML = `<strong>âŒ Erreur :</strong> ${error.message}`;
            } finally {
                loadingDiv.style.display = "none";
            }
        }

        async function showLearningReport() {
            const responseDiv = document.getElementById("response");
            const loadingDiv = document.getElementById("loading");

            loadingDiv.style.display = "block";
            responseDiv.innerHTML = "";

            try {
                const response = await fetch("/learning-report");
                if (!response.ok) throw new Error(`HTTP ${response.status}`);

                const data = await response.json();
                responseDiv.innerHTML = data.report.replace(/\\n/g, '<br>');

            } catch (error) {
                console.error("Error:", error);
                responseDiv.innerHTML = `<strong>âŒ Erreur :</strong> ${error.message}`;
            } finally {
                loadingDiv.style.display = "none";
            }
        }
    </script>
</body>
</html>
'''

with open("/content/templates/index.html", "w", encoding="utf-8") as f:
    f.write(html_content)

templates = Jinja2Templates(directory="/content/templates")

@app.post("/qa")
async def qa_endpoint(
    question: str = Form(""),
    uploaded_file: UploadFile = File(None),
    enable_nlp: bool = Form(False)
):
    """Endpoint avec support NLP"""
    response_text = ""

    if uploaded_file and uploaded_file.filename:
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.filename)[1]) as tmp_file:
                content = await uploaded_file.read()
                tmp_file.write(content)
                tmp_path = tmp_file.name

            new_text_content = extract_text(tmp_path)

            if new_text_content and len(new_text_content.strip()) > 50:
                response_text += f"**âœ… Document '{uploaded_file.filename}' traitÃ©.**\n\n"

                if enable_nlp:
                    doc_analysis = nlp_module.full_analysis(new_text_content[:1000])
                    response_text += f"**ğŸ§  Analyse NLP:** Sentiment: {doc_analysis['sentiment']['label']}, ComplexitÃ©: {doc_analysis['complexity']['level']}\n\n"

            os.unlink(tmp_path)

        except Exception as e:
            response_text += f"**âŒ Erreur fichier:** {str(e)}\n\n"

    if question and question.strip():
        try:
            answer, source, nlp_analysis = answer_question(question, enable_nlp)
            response_text += f"**â“ Question:** {question}\n\n"
            response_text += f"**ğŸ’¡ RÃ©ponse:** {answer}\n\n"
            response_text += f"**ğŸ“š Source:** {source}\n\n"

            if nlp_analysis and enable_nlp:
                response_text += f"**ğŸ§  Analyse NLP:** Sentiment: {nlp_analysis['sentiment']['label']}, ComplexitÃ©: {nlp_analysis['complexity']['level']}\n"

        except Exception as e:
            response_text += f"**âŒ Erreur:** {str(e)}\n\n"

    if not response_text.strip():
        response_text = "**â„¹ï¸ Veuillez poser une question ou uploader un document.**"

    return {"answer": response_text}

@app.get("/learning-report")
async def learning_report():
    """Endpoint pour le rapport d'apprentissage"""
    report = learning_module.analyze_patterns()
    return {"report": report}

@app.get("/", response_class=HTMLResponse)
async def get_index(request: Request):
    return templates.TemplateResponse("index.html", {
        "request": request,
        "faq_count": len(faq_df),
        "interactions_count": len(learning_module.interactions_log),
        "feedback_count": len(learning_module.feedback_log)
    })

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "service": "TuniSpeak API v3.0",
        "features": ["NLP", "ML", "FAQ"],
        "faq_count": len(faq_df),
        "interactions": len(learning_module.interactions_log)
    }

# =============================================================================
# LANCEMENT
# =============================================================================

def launch_gradio():
    print("ğŸš€ Lancement de Gradio...")
    return interface.launch(share=True, quiet=True, inbrowser=False)

def launch_fastapi():
    try:
        os.system("fuser -k 8000/tcp 2>/dev/null || true")
        time.sleep(2)

        print("ğŸš€ Lancement FastAPI...")
        config = uvicorn.Config(app=app, host="0.0.0.0", port=8000, log_level="info")
        server = uvicorn.Server(config)

        try:
            ngrok_token = userdata.get("NGROK_AUTH_TOKEN")
            if ngrok_token:
                ngrok.set_auth_token(ngrok_token)
                public_url = ngrok.connect(8000).public_url
                print(f"ğŸŒ Application: {public_url}")
        except:
            print("ğŸŒ Local: http://localhost:8000")

        loop = asyncio.get_event_loop()
        loop.run_until_complete(server.serve())

    except Exception as e:
        print(f"âŒ Erreur: {e}")

# Tests
print("\n" + "="*60)
print("ğŸ§ª TESTS")
print("="*60)

test_questions = [
    "Comment m'inscrire Ã  l'universitÃ© ?",
    "ÙƒÙŠÙ Ø£Ø³Ø¬Ù„ ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©ØŸ",
    "Quels sont les frais de scolaritÃ© ?"
]

for question in test_questions:
    print(f"\n--- Test: {question} ---")
    answer, source, nlp_analysis = answer_question(question, True)
    print(f"RÃ©ponse: {answer[:100]}...")
    print(f"Source: {source}")
    if nlp_analysis:
        print(f"Sentiment: {nlp_analysis['sentiment']['label']}")

print("\n" + "="*60)
print("âœ… SYSTÃˆME PRÃŠT avec NLP et ML !")
print("="*60)
print(f"ğŸ“Š FAQ: {len(faq_df)} | Interactions: {len(learning_module.interactions_log)}")

gradio_thread = threading.Thread(target=launch_gradio, daemon=True)
gradio_thread.start()

import time
time.sleep(3)
launch_fastapi()